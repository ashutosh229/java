before going into the internal design of hashmap, we should be familiar with some terms:
4. performance

entry interface
-it is a subinterface inside the map interface i.e. we access it using map.entry 
-this subinterface contains key of type K and value of type V 
-since this is an interface and we cannot directly make  an object of it, we implement it and make a class 
called node, which also contains the key of type K and value of type V 
-this node class is nothing but serves to make the node objects which forms the building blocks of map
-when we say map/hashmap, we are talking about a collection of key-value pairs, actually in this, when we talk about 
the internal design, the key-value pairs are the node class objects and the entire collection is an array of these objects 
and this entire array is only internally working as a hashmap 
-the node object contains 4 fields: hash, key, value, next:
    -key represents key of the pair 
    -value represents the value of the pair 
    -hash represents the hash value of the key computed
    -next represents the next key-value pair on the same index and it is a reference variable of type node 
-when we create a hashmap, this array gets created with the initial and default size of 16 if we dont specify the size 
-when we try to insert the key-value pair using the put method:
    -the hash of the key is computed i.e. hash(key) where hash=hash function which is a hashing algorithm, which can be 
    a standard hashing algorithm, or we would have wrote our own custom algorithm as well 
    -the index is calculated by modulating the hash with n i.e. the size of the map (default value of n is 16)
    -this key-value pair is put in that index of the array which has been computed in the previous step
    -if next time, some other insertion happens on the same index, then the next pointer is used to point to the next node 
    in the same index of the array 
-when we try to get the value from the key using the get method:
    -firstly, the hash of the key is computed 
    -then the hash is modulated with size of the map, to find the index 
    -then the index's list is iterated to find the node containing the key by comparing the key and not the hash value (why?,later) 
    -when the key is got, we return the value 
    -when the key is not got, we return false/-1/others

hashCode() and equals():(why not to compare hashcodes and compare the objects)
-if two objects are equal, then their hash codes will be definitely equal 
-if the hash codes of the objects will be equal, then we cannot say whether the objects will be equal or not

why the hashmap can perform insertion/deletion/finding/etc. in O(1) time, on an average ? 
-if we see the internal implementation, then the worst case time complexity is O(N) where N is the size of hashmap 
-but then how can it perform on an average, the operations in constant time complexity 
-because everytime, the number of key-value pairs in the hashmap exceeds a threshold, the entire hashmap doubles it size, 
rehashing occurs, and the data is redistributed and now mostly each node gets inserted into a single index, and now we can fetch 
the details in constant, and again when the threshold crosses, the entire process repeats and this goes on 
-due to the above mentioned process of rehashing, there is nearly constant performance 
-load factor: a constant for the hashmap which determines the threshold for rehashing 
-threshold: load-factor * size of the hash-map 
-rehashing: when the (threshold+1)th key-value pair is inserted into the map, the entire map doubles its size (16-32-64-128-...), and 
for each of the already inserted keys, the hash is again computed and the index is then again computed and then the node is 
inserted on the index 
-does the load factor and rehashing mechanism ensures that there will be least number of collisions?
no, because there can be a possibility that after the rehashing, most of the nodes collide at the same index and therefore, 
this is not the guarantee that there will be no collisions 
-then how to ensure least collisions? 
the solution is treify threshold 
-when the number of nodes on a index exceeds than that of the treify threshold upon insertion, then the linked list of 
the index is converted into a tree (balanced binary search tree)(RBT)(AVL) and therefore the time complexity becomes log(N)

time complexity analysis for hashmap:
-average: O(1) (due to rehashing)
-worst: O(n), for linked list (before treify threshold)
        O(log n), for tree (after treify threshold)
